{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13697018,"sourceType":"datasetVersion","datasetId":8712503},{"sourceId":13698332,"sourceType":"datasetVersion","datasetId":8713474},{"sourceId":13753247,"sourceType":"datasetVersion","datasetId":8751429}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q --no-cache-dir \\\n  numpy==1.26.4 scipy==1.11.4 \\\n  albumentations==1.4.8 albucore==0.0.12 \\\n  opencv-python-headless==4.10.0.84 pycocotools==2.0.10 \\\n  matplotlib==3.8.4 scikit-learn==1.3.2 \\\n  tqdm==4.67.1 bs4==0.0.2 lxml==5.2.2 isort==5.12.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:00:09.902559Z","iopub.execute_input":"2025-11-16T13:00:09.902841Z","iopub.status.idle":"2025-11-16T13:01:07.660782Z","shell.execute_reply.started":"2025-11-16T13:00:09.902797Z","shell.execute_reply":"2025-11-16T13:01:07.659806Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m205.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m204.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m221.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m303.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m286.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m186.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m327.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.3.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip uninstall -y albumentations albucore\n!pip install -q --no-cache-dir albumentations==1.4.8 albucore==0.0.12","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:01:07.662513Z","iopub.execute_input":"2025-11-16T13:01:07.662746Z","iopub.status.idle":"2025-11-16T13:01:12.394100Z","shell.execute_reply.started":"2025-11-16T13:01:07.662722Z","shell.execute_reply":"2025-11-16T13:01:12.393333Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: albumentations 1.4.8\nUninstalling albumentations-1.4.8:\n  Successfully uninstalled albumentations-1.4.8\nFound existing installation: albucore 0.0.12\nUninstalling albucore-0.0.12:\n  Successfully uninstalled albucore-0.0.12\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from pathlib import Path\nimport os\n\ndef resolve_path(local: str, kaggle_dataset: str, *extra: str) -> str:\n    local_path = Path(local, *extra)\n    if local_path.exists():\n        return str(local_path)\n\n    kaggle_base = Path('/kaggle/input') / kaggle_dataset\n    kaggle_path = kaggle_base.joinpath(*extra)\n    if kaggle_path.exists():\n        return str(kaggle_path)\n\n    raise FileNotFoundError(f\"Impossible de trouver {local_path} ni {kaggle_path}\")\n\nis_kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE') is not None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:01:12.397204Z","iopub.execute_input":"2025-11-16T13:01:12.397426Z","iopub.status.idle":"2025-11-16T13:01:12.402951Z","shell.execute_reply.started":"2025-11-16T13:01:12.397402Z","shell.execute_reply":"2025-11-16T13:01:12.402165Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n\nimport json\nimport pickle\nimport xml.etree.ElementTree as ET\nfrom collections import defaultdict\nfrom typing import Dict, List, Tuple\nfrom pathlib import Path\n\nimport albumentations as A\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom albumentations.pytorch import ToTensorV2\nfrom PIL import Image, ImageDraw, ImageFont\nfrom pycocotools.coco import COCO\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.models.detection import _utils as model_utils\nfrom torchvision.models.detection import ssd300_vgg16\nfrom torchvision.models.detection.ssd import SSD300_VGG16_Weights, SSDClassificationHead\nfrom torchvision.models.vgg import VGG16_Weights\nfrom tqdm import tqdm\n\n\n# -----------------------------\n# Datasets\n# -----------------------------\n\nclass COCODataset(Dataset):\n    \"\"\"COCO format dataset returning (image_tensor, target_dict).\"\"\"\n    def __init__(self, root_dir: str, annotation_file: str, transforms=None, min_area: float = 0.0):\n        self.root_dir = root_dir\n        self.coco = COCO(annotation_file)\n        self.image_ids = [img_id for img_id in self.coco.imgs.keys() if len(self.coco.getAnnIds(imgIds=img_id)) > 0]\n        self.cat_ids = self.coco.getCatIds()\n        self.cat_id_to_idx = {cat_id: i + 1 for i, cat_id in enumerate(self.cat_ids)}  # 0 is background\n        self.class_names = [c['name'] for c in self.coco.loadCats(self.cat_ids)]\n        self.transforms = transforms\n        self.min_area = min_area\n        print(f\"COCO dataset loaded: {len(self.image_ids)} images, {len(self.class_names)} classes\")\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx: int):\n        image_id = self.image_ids[idx]\n        img_info = self.coco.loadImgs(image_id)[0]\n        img_path = os.path.join(self.root_dir, img_info['file_name'])\n        image = Image.open(img_path).convert('RGB')\n        w, h = image.size\n\n        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n        anns = self.coco.loadAnns(ann_ids)\n\n        boxes = []\n        labels = []\n        areas = []\n        iscrowd = []\n        for ann in anns:\n            if ann['area'] < self.min_area:\n                continue\n            x, y, bw, bh = ann['bbox']\n            x1, y1, x2, y2 = x, y, x + bw, y + bh\n            # clamp\n            x1 = max(0, min(x1, w - 1))\n            y1 = max(0, min(y1, h - 1))\n            x2 = max(x1 + 1, min(x2, w))\n            y2 = max(y1 + 1, min(y2, h))\n            bw2 = x2 - x1\n            bh2 = y2 - y1\n            area = bw2 * bh2\n            if area < self.min_area or bw2 < 2 or bh2 < 2:\n                continue\n            boxes.append([x1, y1, x2, y2])\n            labels.append(self.cat_id_to_idx[ann['category_id']])\n            areas.append(area)\n            iscrowd.append(ann.get('iscrowd', 0))\n\n        if len(boxes) == 0:\n            # Skip images without usable boxes by picking another index (rare in filtered list)\n            return self.__getitem__((idx + 1) % len(self))\n\n        # Albumentations\n        if self.transforms:\n            transformed = self.transforms(image=np.array(image), bboxes=boxes, labels=labels)\n            img_tensor = transformed['image']\n            boxes = transformed['bboxes']\n            labels = transformed['labels']\n        else:\n            # Basic conversion\n            from torchvision import transforms as T\n            img_tensor = T.ToTensor()(image)\n\n        boxes_tensor = torch.tensor(boxes, dtype=torch.float32)\n        labels_tensor = torch.tensor(labels, dtype=torch.int64)\n        areas_tensor = torch.tensor(areas, dtype=torch.float32)\n        iscrowd_tensor = torch.tensor(iscrowd, dtype=torch.uint8)\n\n        target = {\n            'boxes': boxes_tensor,\n            'labels': labels_tensor,\n            'image_id': torch.tensor([image_id]),\n            'area': areas_tensor,\n            'iscrowd': iscrowd_tensor\n        }\n        return img_tensor, target\n\n\nclass UsureDataset(Dataset):\n    \"\"\"VOC-style XML dataset used in original script.\"\"\"\n    def __init__(self, list_file: str, images_dir: str, transforms=None, class_names: List[str] | None = None, min_area: float = 0.0):\n        if not os.path.isfile(list_file):\n            raise FileNotFoundError(list_file)\n        if not os.path.isdir(images_dir):\n            raise NotADirectoryError(images_dir)\n\n        self.images_dir = images_dir\n        with open(list_file, 'r', encoding='utf-8') as f:\n            self.image_names = [l.strip() for l in f if l.strip()]\n        self.transforms = transforms\n        self.min_area = min_area\n\n        if class_names is None:\n            labels = set()\n            for name in self.image_names:\n                xml_path = os.path.join(images_dir, f\"{name}.xml\")\n                #xml_path = os.path.join(images_dir, \"annotations\", f\"{name}.xml\")\n                if not os.path.isfile(xml_path):\n                    continue\n                try:\n                    root = ET.parse(xml_path).getroot()\n                    for obj in root.findall('object'):\n                        n = obj.findtext('name')\n                        if n:\n                            labels.add(n.strip())\n                except Exception:\n                    continue\n            self.class_names = sorted(labels)\n        else:\n            self.class_names = class_names\n        self.cls_to_idx = {c: i + 1 for i, c in enumerate(self.class_names)}\n        print(f\"Usure dataset loaded: {len(self.image_names)} images, {len(self.class_names)} classes\")\n\n    def __len__(self):\n        return len(self.image_names)\n\n    def _parse_xml(self, xml_path: str, w: int, h: int):\n        boxes, labels, areas, iscrowd = [], [], [], []\n\n        try:\n            root = ET.parse(xml_path).getroot()\n        except Exception:\n            return boxes, labels, areas, iscrowd\n\n        for obj in root.findall('object'):\n            name = obj.findtext('name')\n            if not name or name.strip() not in self.cls_to_idx:\n                continue\n            bnd = obj.find('bndbox')\n            if bnd is None:\n                continue\n\n            try:\n                xmin = float(bnd.findtext('xmin', '0'))\n                ymin = float(bnd.findtext('ymin', '0'))\n                xmax = float(bnd.findtext('xmax', '0'))\n                ymax = float(bnd.findtext('ymax', '0'))\n            except Exception:\n                continue\n            xmin = max(0, min(xmin, w - 1))\n            ymin = max(0, min(ymin, h - 1))\n            xmax = max(xmin + 1, min(xmax, w))\n            ymax = max(ymin + 1, min(ymax, h))\n            bw = xmax - xmin\n            bh = ymax - ymin\n            area = bw * bh\n\n            if area < self.min_area or bw < 2 or bh < 2:\n                continue\n            boxes.append([xmin, ymin, xmax, ymax])\n            labels.append(self.cls_to_idx[name.strip()])\n            areas.append(area)\n            iscrowd.append(0)\n\n        return boxes, labels, areas, iscrowd\n\n    def __getitem__(self, idx: int):\n        name = self.image_names[idx]\n        img_path = os.path.join(self.images_dir, f\"{name}.jpg\")\n        xml_path = os.path.join(self.images_dir, f\"{name}.xml\")\n        #img_path = os.path.join(self.images_dir, \"images\", f\"{name}.jpg\")\n        #xml_path = os.path.join(self.images_dir, \"annotations\", f\"{name}.xml\")\n\n        image = Image.open(img_path).convert('RGB')\n        w, h = image.size\n\n        if os.path.isfile(xml_path):\n            boxes, labels, areas, iscrowd = self._parse_xml(xml_path, w, h)\n        else:\n            boxes, labels, areas, iscrowd = [], [], [], []\n\n        if len(boxes) == 0:\n            # skip empty annotation samples to keep training stable\n            return self.__getitem__((idx + 1) % len(self))\n\n        if self.transforms:\n            transformed = self.transforms(image=np.array(image), bboxes=boxes, labels=labels)\n            img_tensor = transformed['image']\n            boxes = transformed['bboxes']\n            labels = transformed['labels']\n        else:\n            from torchvision import transforms as T\n            img_tensor = T.ToTensor()(image)\n\n        target = {\n            'boxes': torch.tensor(boxes, dtype=torch.float32),\n            'labels': torch.tensor(labels, dtype=torch.int64),\n            'image_id': torch.tensor([idx]),\n            'area': torch.tensor(areas, dtype=torch.float32),\n            'iscrowd': torch.tensor(iscrowd, dtype=torch.uint8)\n        }\n\n        return img_tensor, target\n\n\n# -----------------------------\n# Model utilities\n# -----------------------------\n\ndef build_ssd(num_classes: int, freeze_backbone: bool = False, image_size: int = 300) -> torch.nn.Module:\n    # Download model when calling for the first time\n    model = ssd300_vgg16(weights=SSD300_VGG16_Weights.DEFAULT, weights_backbone=VGG16_Weights.DEFAULT)\n    in_channels = model_utils.retrieve_out_channels(model.backbone, (image_size, image_size))\n    num_anchors = model.anchor_generator.num_anchors_per_location()\n    model.head.classification_head = SSDClassificationHead(\n        in_channels=in_channels,\n        num_anchors=num_anchors,\n        num_classes=num_classes,\n    )\n\n    model.transform.min_size = (image_size,)\n    model.transform.max_size = image_size\n\n    if freeze_backbone:\n        for p in model.backbone.parameters():\n            p.requires_grad = False\n\n    return model\n\n# -----------------------------\n# Training / Validation loops\n# -----------------------------\n\ndef detection_collate(batch: List[Tuple[torch.Tensor, Dict]]):\n    images, targets = list(zip(*batch))\n    # Images already resized to same size, but detection model expects list[Tensor]\n    return list(images), list(targets)\n\ndef train_one_epoch(model, dataloader, optimizer, device, epoch: int):\n    model.train()\n    total, count = 0.0, 0\n\n    pbar = tqdm(dataloader, desc=f\"Train {epoch}\", unit='batch')\n    for images, targets in pbar:\n        images = [img.to(device) for img in images]\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        optimizer.zero_grad()\n\n        try:\n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n            if torch.isfinite(losses):\n                losses.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n                total += losses.item()\n                count += 1\n                pbar.set_postfix(loss=f\"{losses.item():.4f}\", avg=f\"{(total/max(count,1)):.4f}\")\n        except Exception as e:\n            print(f\"Batch error: {e}\")\n            continue\n\n    return total / max(count, 1)\n\n\n# ===== Metrics Helper Functions (dataset agnostic) =====\n\ndef _iou_matrix_np(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    if a.size == 0 or b.size == 0:\n        return np.zeros((a.shape[0], b.shape[0]), dtype=np.float32)\n\n    ax1, ay1, ax2, ay2 = a[:,0], a[:,1], a[:,2], a[:,3]\n    bx1, by1, bx2, by2 = b[:,0], b[:,1], b[:,2], b[:,3]\n\n    inter_x1 = np.maximum(ax1[:,None], bx1[None,:])\n    inter_y1 = np.maximum(ay1[:,None], by1[None,:])\n    inter_x2 = np.minimum(ax2[:,None], bx2[None,:])\n    inter_y2 = np.minimum(ay2[:,None], by2[None,:])\n\n    iw = np.clip(inter_x2 - inter_x1, 0, None)\n    ih = np.clip(inter_y2 - inter_y1, 0, None)\n\n    inter = iw * ih\n    area_a = (ax2 - ax1) * (ay2 - ay1)\n    area_b = (bx2 - bx1) * (by2 - by1)\n    union = area_a[:,None] + area_b[None,:] - inter\n\n    return inter / np.clip(union, 1e-8, None)\n\ndef _integrate_pr(recalls: np.ndarray, precisions: np.ndarray) -> float:\n    if recalls.size == 0:\n        return 0.0\n\n    mrec = np.concatenate(([0.0], recalls, [1.0]))\n    mpre = np.concatenate(([0.0], precisions, [0.0]))\n\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i-1] = max(mpre[i-1], mpre[i])\n    idx = np.where(mrec[1:] != mrec[:-1])[0]\n\n    return float(np.sum((mrec[idx+1] - mrec[idx]) * mpre[idx+1]))\n\ndef _compute_coco_ap(gt_boxes, preds_per_class):\n    \"\"\"\n    gt_boxes: dict[(img_id, cls)] -> list[np.array(4)]\n    preds_per_class: dict[cls] -> list{score, img_id, box}\n    Returns: overall_map, map_per_iou(dict), ap50, ap75\n    \"\"\"\n    iou_thresholds = [round(x/100, 2) for x in range(50, 100, 5)]\n    class_ids = sorted({cls for (_im, cls) in gt_boxes.keys()})\n    gt_count_per_class = {cls: 0 for cls in class_ids}\n    for (img_id, cls), lst in gt_boxes.items():\n        gt_count_per_class[cls] += len(lst)\n\n    ap_per_iou = {thr: [] for thr in iou_thresholds}\n\n    for thr in iou_thresholds:\n        for cls in class_ids:\n            n_gt = gt_count_per_class[cls]\n            if n_gt == 0:\n                continue\n\n            preds = preds_per_class.get(cls, [])\n            preds_sorted = sorted(preds, key=lambda d: d[\"score\"], reverse=True)\n\n            # fresh matched flags per (img, cls)\n            matched_flags = {}\n            for (img_id, c), lst in gt_boxes.items():\n                if c == cls:\n                    matched_flags[(img_id, c)] = [False]*len(lst)\n\n            tp, fp = [], []\n            for pred in preds_sorted:\n                img_id = pred[\"img_id\"]\n                key = (img_id, cls)\n                matched = False\n                if key in gt_boxes:\n                    g = np.vstack(gt_boxes[key])\n                    ious = _iou_matrix_np(pred[\"box\"][None,:], g)[0]\n                    best = np.argmax(ious) if ious.size else -1\n                    if best >= 0 and ious[best] >= thr and not matched_flags[key][best]:\n                        matched_flags[key][best] = True\n                        matched = True\n                tp.append(1 if matched else 0)\n                fp.append(0 if matched else 1)\n\n            if tp:\n                tp_cum = np.cumsum(tp)\n                fp_cum = np.cumsum(fp)\n                recalls = tp_cum / max(1, n_gt)\n                precisions = tp_cum / np.maximum(1, tp_cum + fp_cum)\n                ap = _integrate_pr(recalls, precisions)\n            else:\n                ap = 0.0\n            ap_per_iou[thr].append(ap)\n\n    map_per_iou = {thr: (float(np.mean(v)) if v else 0.0) for thr, v in ap_per_iou.items()}\n    overall_map = float(np.mean(list(map_per_iou.values()))) if map_per_iou else 0.0\n    ap50 = map_per_iou.get(0.5, 0.0)\n    ap75 = map_per_iou.get(0.75, 0.0)\n\n    return overall_map, map_per_iou, ap50, ap75, gt_count_per_class, class_ids\n\ndef _compute_micro_auprc(gt_boxes, preds_per_class, gt_count_per_class, iou_thr=0.5):\n    \"\"\"\n    Micro-averaged AUPRC (area under precision-recall curve) across all classes at a single IoU\n    threshold.\n    \"\"\"\n    total_gt = sum(gt_count_per_class.values())\n    if total_gt == 0:\n        return 0.0\n\n    # Flatten predictions\n    flat = []\n    for cls, plist in preds_per_class.items():\n        for p in plist:\n            flat.append((p[\"score\"], cls, p[\"img_id\"], p[\"box\"]))\n    flat.sort(key=lambda x: x[0], reverse=True)\n\n    matched_flags = {}\n    for k, lst in gt_boxes.items():\n        matched_flags[k] = [False]*len(lst)\n\n    tp_run = 0\n    fp_run = 0\n    precisions = []\n    recalls = []\n    for sc, cls, img_id, box in flat:\n        key = (img_id, cls)\n        matched = False\n        if key in gt_boxes:\n            g = np.vstack(gt_boxes[key])\n            ious = _iou_matrix_np(box[None,:], g)[0]\n            best = np.argmax(ious) if ious.size else -1\n            if best >= 0 and ious[best] >= iou_thr and not matched_flags[key][best]:\n                matched_flags[key][best] = True\n                matched = True\n        if matched:\n            tp_run += 1\n        else:\n            fp_run += 1\n        precisions.append(tp_run / max(1, tp_run + fp_run))\n        recalls.append(tp_run / total_gt)\n\n    if not recalls:\n        return 0.0\n    r = np.array(recalls); p = np.array(precisions)\n    order = np.argsort(r)\n    r = r[order]; p = p[order]\n\n    for i in range(p.size - 1, 0, -1):\n        p[i-1] = max(p[i-1], p[i])\n\n    return float(np.trapz(p, r))\n\ndef _forward_and_collect(model, images, targets, device, gt_boxes, preds_per_class):\n    images = [img.to(device) for img in images]\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n    # loss (train mode)\n    loss_dict = model(images, targets)\n    losses = sum(v for v in loss_dict.values())\n\n    # predictions (eval mode)\n    model.eval()\n    det_outs = model(images)\n    model.train()\n\n    for det, tgt in zip(det_outs, targets):\n        img_id = int(tgt['image_id'].item()) if tgt['image_id'].numel() == 1 else int(tgt['image_id'][0].item())\n\n        # GT\n        gboxes = tgt['boxes'].detach().cpu().numpy()\n        glabels = tgt['labels'].detach().cpu().numpy()\n        for gbox, glab in zip(gboxes, glabels):\n            if glab == 0:\n                continue\n            gt_boxes[(img_id, int(glab))].append(gbox.astype(np.float32))\n\n        # Preds\n        pboxes = det['boxes'].detach().cpu().numpy()\n        pscores = det['scores'].detach().cpu().numpy()\n        plabels = det['labels'].detach().cpu().numpy()\n        for box, sc, lab in zip(pboxes, pscores, plabels):\n            if lab == 0:\n                continue\n            preds_per_class[int(lab)].append({\n                \"score\": float(sc),\n                \"img_id\": img_id,\n                \"box\": box.astype(np.float32)\n            })\n\n    return losses\n\ndef validate(model, dataloader, device, epoch : int | None = None):\n    # Need model in training mode for it to return loss dict (torchvision detection API)\n    was_training = model.training\n    model.train()\n    total_loss = 0.0\n    count = 0\n\n    gt_boxes = defaultdict(list)       # (img_id, cls) -> list[box]\n    preds_per_class = defaultdict(list)\n    \n    if epoch is not None:\n        pbar = tqdm(dataloader, desc=f\"Val   {epoch}\", unit=\"batch\")\n    else:\n        pbar = tqdm(dataloader, desc=\"Val\", unit=\"batch\")\n\n    with torch.no_grad():\n        for images, targets in pbar:\n            if any(t['boxes'].numel() == 0 for t in targets):\n                continue\n            try:\n                losses = _forward_and_collect(model, images, targets, device, gt_boxes, preds_per_class)\n                if torch.isfinite(losses):\n                    total_loss += losses.item()\n                    count += 1\n                    pbar.set_postfix(val_loss=f\"{losses.item():.4f}\", avg=f\"{(total_loss/max(count,1)):.4f}\")\n            except Exception as e:\n                pbar.set_postfix(error=str(e))\n                continue\n\n    # Metrics\n    overall_map, map_per_iou, ap50, ap75, gt_count_per_class, class_ids = _compute_coco_ap(gt_boxes, preds_per_class)\n    auprc = _compute_micro_auprc(gt_boxes, preds_per_class, gt_count_per_class, iou_thr=0.5)\n\n    if epoch is not None:\n        print(f\"[Epoch {epoch}] COCO mAP(0.50:0.95) {overall_map:.4f} | AP50 {ap50:.4f} | AP75 {ap75:.4f} | Micro AUPRC@0.5 {auprc:.4f}\")\n    else:\n        print(f\"COCO mAP(0.50:0.95) {overall_map:.4f} | AP50 {ap50:.4f} | AP75 {ap75:.4f} | Micro AUPRC@0.5 {auprc:.4f}\")\n\n    if not was_training:\n        model.eval()\n\n    #return total_loss / max(count, 1)\n    return total_loss / max(count, 1), gt_boxes, preds_per_class\n\n\n# -----------------------------\n# Prediction & Visualization\n# -----------------------------\n\ndef plot_curves(train_losses: List[float], val_losses: List[float], out_path: str):\n    plt.figure(figsize=(12,5))\n    plt.subplot(1,2,1)\n    plt.plot(train_losses, label='Train')\n    plt.plot(val_losses, label='Val')\n    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss Curves'); plt.grid(True); plt.legend()\n    plt.subplot(1,2,2)\n    plt.plot(train_losses, label='Train')\n    plt.plot(val_losses, label='Val')\n    plt.yscale('log'); plt.xlabel('Epoch'); plt.ylabel('Loss (log)'); plt.title('Loss (Log)'); plt.grid(True); plt.legend()\n    plt.tight_layout()\n    plt.savefig(out_path, dpi=300)\n    plt.close()\n    print(f\"Saved curves to {out_path}\")\n\n# ----- Clean predict_image & visualize -----\ndef predict_image(model,\n                  image_path: str,\n                  device: str,\n                  class_names: List[str],\n                  transform=None,\n                  conf_thresh: float = 0.5):\n    model.eval()\n    pil_image = Image.open(image_path).convert('RGB')\n    orig_w, orig_h = pil_image.size\n\n    if transform is not None:\n        transformed = transform(image=np.array(pil_image), bboxes=[], labels=[])\n        img_tensor = transformed['image']\n    else:\n        basic = A.Compose([\n            A.Resize(300, 300),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            ToTensorV2()\n        ])\n        img_tensor = basic(image=np.array(pil_image))['image']\n\n    resized_h, resized_w = img_tensor.shape[1], img_tensor.shape[2]\n    with torch.no_grad():\n        out = model([img_tensor.to(device)])[0]\n\n    scores = out['scores'].cpu().numpy()\n    boxes = out['boxes'].cpu().numpy()\n    labels = out['labels'].cpu().numpy()\n    keep = scores >= conf_thresh\n    scores, boxes, labels = scores[keep], boxes[keep], labels[keep]\n\n    if (orig_w, orig_h) != (resized_w, resized_h):\n        sx = orig_w / resized_w\n        sy = orig_h / resized_h\n        boxes[:, [0, 2]] *= sx\n        boxes[:, [1, 3]] *= sy\n\n    return {\n        'scores': scores,\n        'boxes': boxes,\n        'labels': labels,\n        'orig_size': (orig_w, orig_h),\n        'proc_size': (resized_w, resized_h)\n    }\n\n\ndef visualize_predictions(image_path: str, predictions: Dict, class_names: List[str], save_path: str | None = None):\n    image = Image.open(image_path).convert('RGB')\n    draw = ImageDraw.Draw(image)\n\n    try:\n        font = ImageFont.truetype(\"arial.ttf\", 16)\n    except Exception:\n        font = ImageFont.load_default()\n    colors = ['red','blue','green','orange','purple','yellow','cyan','magenta','lime','pink']\n\n    for box, label, score in zip(predictions['boxes'], predictions['labels'], predictions['scores']):\n        x1, y1, x2, y2 = box\n        cname = class_names[label - 1] if 0 < label <= len(class_names) else f\"cls_{label}\"\n        color = colors[label % len(colors)]\n        # outline\n        draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n\n        text = f\"{cname}:{score:.2f}\"\n        # Proper bbox for text\n        tx1, ty1, tx2, ty2 = draw.textbbox((x1, y1), text, font=font)\n        # Shift label above box if space, else draw inside\n        label_bottom = y1\n        label_top = label_bottom - (ty2 - ty1)\n        if label_top < 0:\n            label_top = y1\n            label_bottom = y1 + (ty2 - ty1)\n        draw.rectangle([tx1, label_top, tx2, label_bottom], fill=color)\n        draw.text((tx1, label_top), text, fill='white', font=font)\n\n    if save_path:\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        image.save(save_path)\n        print(f\"Saved prediction image -> {save_path}\")\n    return image\n\n\n# -----------------------------\n# Wrapper combining training & validation with curve saving\n# -----------------------------\n\ndef train_and_evaluate(model: torch.nn.Module,\n                       train_loader: DataLoader,\n                       val_loader: DataLoader,\n                       optimizer: torch.optim.Optimizer,\n                       scheduler: torch.optim.lr_scheduler._LRScheduler | None,\n                       device: str,\n                       epochs: int,\n                       output_dir: str,\n                       prefix: str):\n    os.makedirs(output_dir, exist_ok=True)\n    train_losses: List[float] = []\n    val_losses: List[float] = []\n    \n    best_val_loss = float('inf')\n    patience = 5  # number of epochs without improvment (otherwise it stops)\n    best_epoch = 0\n    trigger_times = 0\n\n    for epoch in range(epochs):\n        tr = train_one_epoch(model, train_loader, optimizer, device, epoch + 1)\n        #va = validate(model, val_loader, device, epoch + 1)\n        va_loss, _, _ = validate(model, val_loader, device, epoch + 1)\n        va = va_loss \n\n        if scheduler:\n            scheduler.step()\n        train_losses.append(tr)\n        val_losses.append(va)\n\n        # Add of Early Stopping to avoid overfitting\n        if va < best_val_loss:\n            best_val_loss = va\n            best_epoch = epoch + 1\n            trigger_times = 0\n            # Save of the best model\n            torch.save({'model_state_dict': model.state_dict()},os.path.join(output_dir, f'{prefix}_best.pth'))\n            print(f\" New best model at epoch {epoch+1} (val_loss={va:.4f}) saved.\")\n        else:\n            trigger_times += 1\n            print(f\" No improvement for {trigger_times} epoch(s).\")\n\n        if trigger_times >= patience:\n            print(f\" Early stopping at epoch {epoch+1}. Best val_loss = {best_val_loss:.4f}\")\n            break\n        \n        # intermediate checkpoint every 5 epochs\n        if (epoch + 1) % 5 == 0 or (epoch + 1) == epochs:\n            torch.save({'model_state_dict': model.state_dict()}, os.path.join(output_dir, f'{prefix}_epoch_{epoch+1}.pth'))\n\n    if len(train_losses) > 0:\n        plot_curves(train_losses, val_losses, os.path.join(output_dir, f'{prefix}_curves.png'))\n        torch.save({'model_state_dict': model.state_dict()}, os.path.join(output_dir, f'{prefix}_final.pth'))\n        print(f\"\\n Training completed after {epoch+1} epochs. Best val_loss={best_val_loss:.4f} (epoch {best_epoch})\")\n\n    return train_losses, val_losses\n\n\n# -----------------------------\n# Main pipeline\n# -----------------------------\n\ndef main():\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print(f\"Using device: {device}\")\n\n    # ======================================================\n    # COCO DATASET\n    # ======================================================\n    print(\"\\n=== Loading COCO subset ===\")\n\n    # Paths (subset versions for faster experimentation)\n    # Use Kaggle input directory if available, otherwise fall back to local layout\n    kaggle_root = Path('/kaggle/input/coco2014/subset_coco')\n    local_root = Path('subset_coco')\n    coco_root = kaggle_root if kaggle_root.exists() else local_root\n\n    train_images_dir = str(coco_root / 'subset_train_images_2014')\n    val_images_dir   = str(coco_root / 'subset_valid_images_2014')\n    test_images_dir  = str(coco_root / 'test_image_info_2014')\n    train_ann        = str(coco_root / 'train_val_annotations' / 'subset_instances_train2014.json')\n    val_ann          = str(coco_root / 'train_val_annotations' / 'subset_instances_val2014.json')\n    test_image_name  = 'COCO_test2014_000000000001.jpg'\n    # prefer writing to Kaggle working directory when available\n    saved_model_path = '/kaggle/working/outputs/ssd300_coco_final.pth' if Path('/kaggle/working').exists() else 'outputs/ssd300_coco_final.pth'\n\n    # Inline transforms (train / val)\n    image_size = 300\n\n    train_transform = A.Compose([\n        A.Resize(image_size, image_size),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2()\n    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], min_area=1, min_visibility=0.0))\n\n    val_transform = A.Compose([\n        A.Resize(image_size, image_size),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2()\n    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], min_area=1, min_visibility=0.0))\n\n    # Datasets / loaders\n    coco_train = COCODataset(train_images_dir, train_ann, transforms=train_transform, min_area=32*32)\n    coco_val   = COCODataset(val_images_dir,   val_ann,   transforms=val_transform,   min_area=32*32)\n    num_classes_coco = len(coco_train.class_names) + 1  # + background\n    print(f\"COCO classes ({num_classes_coco-1}): {coco_train.class_names}\")\n\n    train_loader = DataLoader(coco_train, batch_size=8, shuffle=True,  collate_fn=detection_collate, num_workers=0)\n    val_loader   = DataLoader(coco_val,   batch_size=8, shuffle=False, collate_fn=detection_collate, num_workers=0)\n\n    # Model + optim\n    model = build_ssd(num_classes_coco, freeze_backbone=True).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n\n    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 15], gamma=0.1)\n\n    # Training epochs\n    epochs_coco = 1\n\n    if epochs_coco > 0:\n        train_and_evaluate(model, train_loader, val_loader, optimizer, scheduler, device, epochs_coco, 'outputs', 'ssd300_coco')\n\n    # prediction on a test image\n    test_image_path = os.path.join(test_images_dir, test_image_name)\n    if os.path.exists(test_image_path):\n        print(\"COCO prediction test...\")\n        preds = predict_image(model, test_image_path, device, coco_train.class_names,\n                              transform=val_transform, conf_thresh=0.7)\n        visualize_predictions(test_image_path, preds, coco_train.class_names, save_path='outputs/prediction_coco.jpg')\n        print(f\"Detections: {len(preds['labels'])}\")\n        for l, s in zip(preds['labels'], preds['scores']):\n            cname = coco_train.class_names[l - 1] if l > 0 else 'bg'\n            print(f\"  {cname}: {s:.3f}\")\n\n    # inference using a checkpoint\n    model_checkpoint_path = saved_model_path\n    model = build_ssd(num_classes_coco, freeze_backbone=True).to(device)\n    if os.path.isfile(model_checkpoint_path):\n        model.load_state_dict(torch.load(model_checkpoint_path, map_location=device)['model_state_dict'])\n        print(f\"Loaded checkpoint from {model_checkpoint_path}\")\n    \n    #val_loss = validate(model, val_loader, device)\n    val_loss, _, _ = validate(model, val_loader, device)\n\n\n    # ======================================================\n    # USURE DATASET\n    # ======================================================\n    print(\"\\n=== Loading Usure dataset ===\")\n\n    # Usure dataset: prefer Kaggle input when available\n    usure_slug = 'data-tu-aug/data_tu'\n    local_usure = Path('data_tu')\n    kaggle_usure = Path('/kaggle/input') / usure_slug\n    usure_root = kaggle_usure if kaggle_usure.exists() else local_usure\n\n    usure_train_list = str(usure_root / 'image_names_train.txt')\n    usure_val_list   = str(usure_root / 'image_names_val.txt')\n    usure_test_list  = str(usure_root / 'image_names_test.txt')\n\n    \n    usure_images_dir = str(usure_root / 'combined')\n\n    # Load class filter (optional) from parameters (prefer Kaggle dataset if present)\n    param_slug = 'utilities'\n    local_params = Path('utilities/parameters.json')\n    kaggle_params = Path('/kaggle/input') / param_slug / 'parameters.json'\n    param_path = kaggle_params if kaggle_params.exists() else local_params\n    print(\"[Info] Param path:\", param_path)\n\n    with open(param_path, 'r', encoding='utf-8') as f:\n        params = json.load(f)\n    class_filter = params.get('class_filter', None)\n\n    # Datasets / loaders\n    usure_train = UsureDataset(usure_train_list, usure_images_dir, transforms=train_transform, class_names=class_filter, min_area=32*32)\n    usure_val   = UsureDataset(usure_val_list,   usure_images_dir, transforms=val_transform,   class_names=class_filter, min_area=32*32)\n    usure_test  = UsureDataset(usure_test_list,  usure_images_dir, transforms=val_transform,   class_names=class_filter,min_area=32*32)\n\n    num_usure_classes = len(usure_train.class_names) + 1\n    print(f\"Usure classes ({num_usure_classes-1}): {usure_train.class_names}\")\n\n    usure_train_loader = DataLoader(usure_train, batch_size=8, shuffle=True,  collate_fn=detection_collate, num_workers=0)\n    usure_val_loader   = DataLoader(usure_val,   batch_size=8, shuffle=False, collate_fn=detection_collate, num_workers=0)\n    usure_test_loader  = DataLoader(usure_test,  batch_size=8, shuffle=False, collate_fn=detection_collate, num_workers=0)\n\n\n    # Build a fresh model for Usure\n    model_usure = build_ssd(num_usure_classes, freeze_backbone=False).to(device)\n\n    optimizer_u = torch.optim.AdamW(model_usure.parameters(), lr=1e-4, weight_decay=1e-4)\n    scheduler_u = torch.optim.lr_scheduler.MultiStepLR(optimizer_u, milestones=[10, 15], gamma=0.1)\n\n    # Short demo run; extend epochs for real training\n    epochs_usure = 30\n    train_and_evaluate(model_usure, usure_train_loader, usure_val_loader, optimizer_u, scheduler_u, device, epochs_usure, 'outputs', 'ssd300_usure')\n\n    # Single prediction on first validation sample\n    with open(usure_val_list, 'r', encoding='utf-8') as f:\n        first_name = f.readline().strip()\n    usure_test_image = os.path.join(usure_images_dir, f\"{first_name}.jpg\")\n    if os.path.exists(usure_test_image):\n        print(\"Usure prediction test...\")\n        preds_u = predict_image(model_usure, usure_test_image, device, usure_train.class_names,\n                                transform=val_transform, conf_thresh=0.2)\n        visualize_predictions(usure_test_image, preds_u, usure_train.class_names, save_path='outputs/prediction_usure_aug.jpg')\n        print(f\"Usure detections: {len(preds_u['labels'])}\")\n        for l, s in zip(preds_u['labels'], preds_u['scores']):\n            cname = usure_train.class_names[l - 1] if l > 0 else 'bg'\n            print(f\"  {cname}: {s:.3f}\")\n            \n    print(\"\\n=== Evaluation finale sur TEST SET ===\")\n\n    test_loss, gt_boxes, preds_per_class = validate(model_usure, usure_test_loader, device)\n    print(f\"Test loss = {test_loss:.4f}\")\n    \n    os.makedirs(\"outputs\", exist_ok=True)\n    \n    with open(\"outputs/gt_boxes_aug.pkl\", \"wb\") as f:\n        pickle.dump(gt_boxes, f)\n    \n    with open(\"outputs/preds_per_class_aug.pkl\", \"wb\") as f:\n        pickle.dump(preds_per_class, f)\n    \n    with open(\"outputs/class_names_aug.json\", \"w\") as f:\n        json.dump(usure_train.class_names, f)\n\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:01:12.404781Z","iopub.execute_input":"2025-11-16T13:01:12.405127Z","iopub.status.idle":"2025-11-16T13:39:55.926891Z","shell.execute_reply.started":"2025-11-16T13:01:12.405098Z","shell.execute_reply":"2025-11-16T13:39:55.925981Z"}},"outputs":[{"name":"stderr","text":"INFO:albumentations.check_version:A new version of Albumentations is available: 2.0.8 (you have 1.4.8). Upgrade using: pip install --upgrade albumentations\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n\n=== Loading COCO subset ===\nloading annotations into memory...\nDone (t=2.36s)\ncreating index...\nindex created!\nCOCO dataset loaded: 8208 images, 80 classes\nloading annotations into memory...\nDone (t=1.17s)\ncreating index...\nindex created!\nCOCO dataset loaded: 4013 images, 80 classes\nCOCO classes (80): ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/ssd300_vgg16_coco-b556d3b4.pth\" to /root/.cache/torch/hub/checkpoints/ssd300_vgg16_coco-b556d3b4.pth\n100%|██████████| 136M/136M [00:00<00:00, 209MB/s]  \nTrain 1: 100%|██████████| 1026/1026 [04:57<00:00,  3.45batch/s, avg=7.9586, loss=4.4590] \nVal   1: 100%|██████████| 502/502 [04:30<00:00,  1.86batch/s, avg=4.8571, val_loss=3.9487]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] COCO mAP(0.50:0.95) 0.1592 | AP50 0.3774 | AP75 0.0984 | Micro AUPRC@0.5 0.4824\n New best model at epoch 1 (val_loss=4.8571) saved.\nSaved curves to outputs/ssd300_coco_curves.png\n\n Training completed after 1 epochs. Best val_loss=4.8571 (epoch 1)\nLoaded checkpoint from /kaggle/working/outputs/ssd300_coco_final.pth\n","output_type":"stream"},{"name":"stderr","text":"Val: 100%|██████████| 502/502 [04:04<00:00,  2.05batch/s, avg=4.8571, val_loss=3.9487]\n","output_type":"stream"},{"name":"stdout","text":"COCO mAP(0.50:0.95) 0.1592 | AP50 0.3774 | AP75 0.0984 | Micro AUPRC@0.5 0.4824\n\n=== Loading Usure dataset ===\n[Info] Param path: /kaggle/input/utilities/parameters.json\nUsure dataset loaded: 1151 images, 8 classes\nUsure dataset loaded: 288 images, 8 classes\nUsure dataset loaded: 141 images, 8 classes\nUsure classes (8): ['temoin:0', 'temoin:25', 'temoin:50', 'temoin:75', 'temoin:80', 'temoin:90', 'temoin:95', 'temoin:100']\n","output_type":"stream"},{"name":"stderr","text":"Train 1: 100%|██████████| 144/144 [01:09<00:00,  2.06batch/s, avg=7.2537, loss=3.8349] \nVal   1: 100%|██████████| 36/36 [00:17<00:00,  2.04batch/s, avg=4.1376, val_loss=4.3485]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] COCO mAP(0.50:0.95) 0.1369 | AP50 0.2706 | AP75 0.1166 | Micro AUPRC@0.5 0.4746\n New best model at epoch 1 (val_loss=4.1376) saved.\n","output_type":"stream"},{"name":"stderr","text":"Train 2: 100%|██████████| 144/144 [00:53<00:00,  2.69batch/s, avg=3.2227, loss=2.7823]\nVal   2: 100%|██████████| 36/36 [00:13<00:00,  2.71batch/s, avg=3.1200, val_loss=3.6776]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] COCO mAP(0.50:0.95) 0.2696 | AP50 0.4722 | AP75 0.3098 | Micro AUPRC@0.5 0.7125\n New best model at epoch 2 (val_loss=3.1200) saved.\n","output_type":"stream"},{"name":"stderr","text":"Train 3: 100%|██████████| 144/144 [00:53<00:00,  2.70batch/s, avg=2.3613, loss=2.7585]\nVal   3: 100%|██████████| 36/36 [00:13<00:00,  2.74batch/s, avg=2.7488, val_loss=3.3796]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] COCO mAP(0.50:0.95) 0.2693 | AP50 0.4468 | AP75 0.3246 | Micro AUPRC@0.5 0.7531\n New best model at epoch 3 (val_loss=2.7488) saved.\n","output_type":"stream"},{"name":"stderr","text":"Train 4: 100%|██████████| 144/144 [00:53<00:00,  2.70batch/s, avg=1.8162, loss=1.7236]\nVal   4: 100%|██████████| 36/36 [00:13<00:00,  2.73batch/s, avg=2.4821, val_loss=2.9219]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 4] COCO mAP(0.50:0.95) 0.3939 | AP50 0.6298 | AP75 0.4366 | Micro AUPRC@0.5 0.8485\n New best model at epoch 4 (val_loss=2.4821) saved.\n","output_type":"stream"},{"name":"stderr","text":"Train 5: 100%|██████████| 144/144 [00:53<00:00,  2.70batch/s, avg=1.5224, loss=1.0976]\nVal   5: 100%|██████████| 36/36 [00:13<00:00,  2.74batch/s, avg=2.5886, val_loss=2.4862]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 5] COCO mAP(0.50:0.95) 0.3460 | AP50 0.5603 | AP75 0.4167 | Micro AUPRC@0.5 0.7829\n No improvement for 1 epoch(s).\n","output_type":"stream"},{"name":"stderr","text":"Train 6: 100%|██████████| 144/144 [00:52<00:00,  2.74batch/s, avg=1.3273, loss=0.9947]\nVal   6: 100%|██████████| 36/36 [00:13<00:00,  2.71batch/s, avg=2.3505, val_loss=2.6958]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 6] COCO mAP(0.50:0.95) 0.4393 | AP50 0.6713 | AP75 0.5038 | Micro AUPRC@0.5 0.8545\n New best model at epoch 6 (val_loss=2.3505) saved.\n","output_type":"stream"},{"name":"stderr","text":"Train 7: 100%|██████████| 144/144 [00:52<00:00,  2.76batch/s, avg=1.0584, loss=0.9723]\nVal   7: 100%|██████████| 36/36 [00:13<00:00,  2.73batch/s, avg=2.3466, val_loss=2.7009]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 7] COCO mAP(0.50:0.95) 0.4769 | AP50 0.7185 | AP75 0.6335 | Micro AUPRC@0.5 0.8755\n New best model at epoch 7 (val_loss=2.3466) saved.\n","output_type":"stream"},{"name":"stderr","text":"Train 8: 100%|██████████| 144/144 [00:52<00:00,  2.75batch/s, avg=0.9410, loss=0.6416]\nVal   8: 100%|██████████| 36/36 [00:13<00:00,  2.74batch/s, avg=2.2779, val_loss=2.2782]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 8] COCO mAP(0.50:0.95) 0.4991 | AP50 0.7981 | AP75 0.5844 | Micro AUPRC@0.5 0.8912\n New best model at epoch 8 (val_loss=2.2779) saved.\n","output_type":"stream"},{"name":"stderr","text":"Train 9: 100%|██████████| 144/144 [00:52<00:00,  2.76batch/s, avg=0.7805, loss=1.2203]\nVal   9: 100%|██████████| 36/36 [00:13<00:00,  2.74batch/s, avg=2.3565, val_loss=2.5405]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 9] COCO mAP(0.50:0.95) 0.5183 | AP50 0.7787 | AP75 0.6315 | Micro AUPRC@0.5 0.8950\n No improvement for 1 epoch(s).\n","output_type":"stream"},{"name":"stderr","text":"Train 10: 100%|██████████| 144/144 [00:52<00:00,  2.75batch/s, avg=0.6980, loss=0.6479]\nVal   10: 100%|██████████| 36/36 [00:13<00:00,  2.74batch/s, avg=2.3935, val_loss=2.4202]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 10] COCO mAP(0.50:0.95) 0.4788 | AP50 0.7494 | AP75 0.5978 | Micro AUPRC@0.5 0.8853\n No improvement for 2 epoch(s).\n","output_type":"stream"},{"name":"stderr","text":"Train 11: 100%|██████████| 144/144 [00:52<00:00,  2.76batch/s, avg=0.4313, loss=0.2827]\nVal   11: 100%|██████████| 36/36 [00:13<00:00,  2.74batch/s, avg=2.2016, val_loss=2.1182]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 11] COCO mAP(0.50:0.95) 0.5919 | AP50 0.8340 | AP75 0.6998 | Micro AUPRC@0.5 0.9055\n New best model at epoch 11 (val_loss=2.2016) saved.\n","output_type":"stream"},{"name":"stderr","text":"Train 12: 100%|██████████| 144/144 [00:53<00:00,  2.71batch/s, avg=0.2590, loss=0.2852]\nVal   12: 100%|██████████| 36/36 [00:13<00:00,  2.74batch/s, avg=2.2779, val_loss=2.3192]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 12] COCO mAP(0.50:0.95) 0.5992 | AP50 0.8270 | AP75 0.7606 | Micro AUPRC@0.5 0.9045\n No improvement for 1 epoch(s).\n","output_type":"stream"},{"name":"stderr","text":"Train 13: 100%|██████████| 144/144 [00:52<00:00,  2.75batch/s, avg=0.1938, loss=0.1379]\nVal   13: 100%|██████████| 36/36 [00:13<00:00,  2.74batch/s, avg=2.4159, val_loss=2.4724]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 13] COCO mAP(0.50:0.95) 0.5931 | AP50 0.8185 | AP75 0.6898 | Micro AUPRC@0.5 0.9004\n No improvement for 2 epoch(s).\n","output_type":"stream"},{"name":"stderr","text":"Train 14: 100%|██████████| 144/144 [00:52<00:00,  2.74batch/s, avg=0.1547, loss=0.2007]\nVal   14: 100%|██████████| 36/36 [00:13<00:00,  2.73batch/s, avg=2.5148, val_loss=2.6277]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 14] COCO mAP(0.50:0.95) 0.5900 | AP50 0.8203 | AP75 0.7559 | Micro AUPRC@0.5 0.9032\n No improvement for 3 epoch(s).\n","output_type":"stream"},{"name":"stderr","text":"Train 15: 100%|██████████| 144/144 [00:52<00:00,  2.75batch/s, avg=0.1299, loss=0.0841]\nVal   15: 100%|██████████| 36/36 [00:13<00:00,  2.74batch/s, avg=2.5973, val_loss=2.7065]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 15] COCO mAP(0.50:0.95) 0.5984 | AP50 0.8194 | AP75 0.7548 | Micro AUPRC@0.5 0.9014\n No improvement for 4 epoch(s).\n","output_type":"stream"},{"name":"stderr","text":"Train 16: 100%|██████████| 144/144 [00:52<00:00,  2.74batch/s, avg=0.1072, loss=0.1026]\nVal   16: 100%|██████████| 36/36 [00:13<00:00,  2.73batch/s, avg=2.6340, val_loss=2.7652]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 16] COCO mAP(0.50:0.95) 0.5983 | AP50 0.8174 | AP75 0.7487 | Micro AUPRC@0.5 0.9002\n No improvement for 5 epoch(s).\n Early stopping at epoch 16. Best val_loss = 2.2016\nSaved curves to outputs/ssd300_usure_curves.png\n\n Training completed after 16 epochs. Best val_loss=2.2016 (epoch 11)\nUsure prediction test...\nSaved prediction image -> outputs/prediction_usure_aug.jpg\nUsure detections: 1\n  temoin:100: 0.792\n\n=== Evaluation finale sur TEST SET ===\n","output_type":"stream"},{"name":"stderr","text":"Val: 100%|██████████| 18/18 [00:09<00:00,  1.92batch/s, avg=2.2661, val_loss=0.0502]","output_type":"stream"},{"name":"stdout","text":"COCO mAP(0.50:0.95) 0.6944 | AP50 0.8859 | AP75 0.7604 | Micro AUPRC@0.5 0.8829\nTest loss = 2.2661\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport json\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict\nfrom pathlib import Path\n\n# Helper to resolve input files either locally, in /kaggle/working, or under /kaggle/input\ndef resolve_input(rel_path: str) -> Path:\n    p = Path(rel_path)\n    if p.exists():\n        return p\n    p_work = Path('/kaggle/working') / rel_path\n    if p_work.exists():\n        return p_work\n    kaggle_input = Path('/kaggle/input')\n    if kaggle_input.exists():\n        # search for a file with the same name under /kaggle/input\n        name = p.name\n        matches = list(kaggle_input.rglob(name))\n        if matches:\n            return matches[0]\n    raise FileNotFoundError(f\"Input file not found: tried {p}, {p_work}, and under /kaggle/input\")\n\n# Helper to choose an output directory (prefer /kaggle/working when available)\ndef resolve_output_dir(dir_name: str = 'metrics') -> Path:\n    work = Path('/kaggle/working')\n    if work.exists():\n        out = work / dir_name\n    else:\n        out = Path(dir_name)\n    out.mkdir(parents=True, exist_ok=True)\n    return out\n\n# ============================================================\n# IOU function\n# ============================================================\ndef iou_matrix(a, b):\n    if len(a) == 0 or len(b) == 0:\n        return np.zeros((len(a), len(b)))\n    a = np.asarray(a); b = np.asarray(b)\n\n    ax1, ay1, ax2, ay2 = a[:,0], a[:,1], a[:,2], a[:,3]\n    bx1, by1, bx2, by2 = b[:,0], b[:,1], b[:,2], b[:,3]\n\n    inter_x1 = np.maximum(ax1[:,None], bx1[None,:])\n    inter_y1 = np.maximum(ay1[:,None], by1[None,:])\n    inter_x2 = np.minimum(ax2[:,None], bx2[None,:])\n    inter_y2 = np.minimum(ay2[:,None], by2[None,:])\n\n    inter_w = np.clip(inter_x2 - inter_x1, 0, None)\n    inter_h = np.clip(inter_y2 - inter_y1, 0, None)\n    inter = inter_w * inter_h\n\n    area_a = (ax2 - ax1) * (ay2 - ay1)\n    area_b = (bx2 - bx1) * (by2 - by1)\n    union = area_a[:,None] + area_b[None,:] - inter\n\n    return inter / np.clip(union, 1e-8, None)\n\n\n# ============================================================\n# Compute AP from precision/recall\n# ============================================================\ndef compute_ap(rec, prec):\n    mrec = np.concatenate(([0], rec, [1]))\n    mpre = np.concatenate(([0], prec, [0]))\n\n    for i in range(len(mpre)-1, 0, -1):\n        mpre[i-1] = max(mpre[i-1], mpre[i])\n\n    idx = np.where(mrec[1:] != mrec[:-1])[0]\n    return float(np.sum((mrec[idx+1] - mrec[idx]) * mpre[idx+1]))\n\n\n# ============================================================\n# MAIN\n# ============================================================\ndef main():\n\n    # Load all stored results (resolve file locations for Kaggle)\n    gt_path = resolve_input(\"outputs/gt_boxes_aug.pkl\")\n    preds_path = resolve_input(\"outputs/preds_per_class_aug.pkl\")\n    names_path = resolve_input(\"outputs/class_names_aug.json\")\n\n    with open(gt_path, \"rb\") as f:\n        gt_boxes = pickle.load(f)\n\n    with open(preds_path, \"rb\") as f:\n        preds_all = pickle.load(f)\n\n    with open(names_path, \"r\") as f:\n        class_names = json.load(f)\n\n    n_classes = len(class_names)\n    print(f\"✔ Loaded {n_classes} classes\")\n\n    # choose output metrics dir (prefer /kaggle/working/metrics)\n    metrics_dir = resolve_output_dir('metrics')\n    print(f\"Metrics will be written to: {metrics_dir}\")\n\n    # ============================================================\n    # CONFUSION MATRIX\n    # ============================================================\n    cm = np.zeros((n_classes, n_classes), dtype=int)\n\n    for cls_pred_label, preds in preds_all.items():\n        cls_pred = cls_pred_label - 1  # convert 1→0 index\n\n        for p in preds:\n            img_id = p[\"img_id\"]\n            box_pred = p[\"box\"]\n\n            # find matching GT in same image\n            best_gt_cls = None\n            best_iou = 0\n            for (img_gt, cls_gt), gt_list in gt_boxes.items():\n                if img_gt == img_id:\n                    ious = iou_matrix([box_pred], gt_list)[0]\n                    idx = np.argmax(ious)\n                    if ious[idx] >= 0.5:\n                        best_gt_cls = cls_gt - 1\n                        best_iou = ious[idx]\n                        break\n\n            if best_gt_cls is not None:\n                cm[best_gt_cls, cls_pred] += 1\n\n    plt.figure(figsize=(10,8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n                xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel(\"Prediction\")\n    plt.ylabel(\"Ground truth\")\n    plt.title(\"Confusion Matrix (IoU>0.5)\")\n    plt.tight_layout()\n    cm_path = metrics_dir / \"confusion_matrix_aug.png\"\n    plt.savefig(cm_path, dpi=300)\n    plt.close()\n    print(f\"✔ Saved {cm_path}\")\n\n    # ============================================================\n    # AP per class for IoU thresholds\n    # ============================================================\n    iou_thresholds = [0.5, 0.75] + [round(x/100, 2) for x in range(50, 100, 5)]\n    ap_per_iou = {thr: [] for thr in iou_thresholds}\n\n    # compute GT count per class\n    gt_count = {c: 0 for c in range(1, n_classes+1)}\n    for (_, cls), lst in gt_boxes.items():\n        gt_count[cls] += len(lst)\n\n    # loop IoU thresholds\n    for thr in iou_thresholds:\n        for cls in range(1, n_classes+1):\n\n            preds = sorted(preds_all.get(cls, []), key=lambda x: x[\"score\"], reverse=True)\n            tp, fp = [], []\n            matched = defaultdict(set)\n            total_gt = gt_count[cls]\n\n            for p in preds:\n                img_id = p[\"img_id\"]\n                box_pred = p[\"box\"]\n\n                # find GT for that class in same img\n                gt_list = gt_boxes.get((img_id, cls), [])\n                match = False\n\n                if len(gt_list) > 0:\n                    ious = iou_matrix([box_pred], gt_list)[0]\n                    idx = np.argmax(ious)\n                    if ious[idx] >= thr and idx not in matched[(img_id, cls)]:\n                        match = True\n                        matched[(img_id, cls)].add(idx)\n\n                tp.append(1 if match else 0)\n                fp.append(0 if match else 1)\n\n            if total_gt == 0:\n                ap_per_iou[thr].append(0)\n                continue\n\n            tp = np.cumsum(tp)\n            fp = np.cumsum(fp)\n\n            rec = tp / total_gt\n            prec = tp / np.maximum(tp + fp, 1e-8)\n            ap_per_iou[thr].append(compute_ap(rec, prec))\n\n    # mAP summary\n    map_50_95 = np.mean([np.mean(v) for k,v in ap_per_iou.items() if k >= 0.5 and k <= 0.95])\n    ap50 = np.mean(ap_per_iou[0.5])\n    ap75 = np.mean(ap_per_iou[0.75])\n\n    # ============================================================\n    # SAVE JSON REPORT\n    # ============================================================\n    report = {\n        \"mAP_50_95\": map_50_95,\n        \"AP50\": ap50,\n        \"AP75\": ap75,\n        \"per_class_AP50\": {class_names[i]: float(ap_per_iou[0.5][i]) for i in range(n_classes)},\n        \"per_class_AP75\": {class_names[i]: float(ap_per_iou[0.75][i]) for i in range(n_classes)},\n        \"num_ground_truth\": sum(gt_count.values()),\n        \"num_predictions\": sum(len(v) for v in preds_all.values()),\n        \"class_names\": class_names\n    }\n\n    out_json = metrics_dir / \"final_metrics_aug.json\"\n    with open(out_json, \"w\") as f:\n        json.dump(report, f, indent=4)\n\n    print(f\"✔ Saved {out_json}\")\n    print(\"\\n===== SUMMARY =====\")\n    print(json.dumps(report, indent=4))\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:39:55.927969Z","iopub.execute_input":"2025-11-16T13:39:55.928362Z","iopub.status.idle":"2025-11-16T13:39:57.346238Z","shell.execute_reply.started":"2025-11-16T13:39:55.928342Z","shell.execute_reply":"2025-11-16T13:39:57.345586Z"}},"outputs":[{"name":"stderr","text":"INFO:numexpr.utils:NumExpr defaulting to 4 threads.\n","output_type":"stream"},{"name":"stdout","text":"✔ Loaded 8 classes\nMetrics will be written to: /kaggle/working/metrics\n✔ Saved /kaggle/working/metrics/confusion_matrix_aug.png\n✔ Saved /kaggle/working/metrics/final_metrics_aug.json\n\n===== SUMMARY =====\n{\n    \"mAP_50_95\": 0.6944433591636948,\n    \"AP50\": 0.8858521827967646,\n    \"AP75\": 0.7603748617253773,\n    \"per_class_AP50\": {\n        \"temoin:0\": 0.5714285714285714,\n        \"temoin:25\": 1.0,\n        \"temoin:50\": 1.0,\n        \"temoin:75\": 0.9728535353535352,\n        \"temoin:80\": 0.7544262181616832,\n        \"temoin:90\": 0.8955471539592522,\n        \"temoin:95\": 0.8925619834710744,\n        \"temoin:100\": 1.0\n    },\n    \"per_class_AP75\": {\n        \"temoin:0\": 0.39285714285714285,\n        \"temoin:25\": 1.0,\n        \"temoin:50\": 1.0,\n        \"temoin:75\": 0.8956930415263749,\n        \"temoin:80\": 0.521766276762643,\n        \"temoin:90\": 0.7469248568992816,\n        \"temoin:95\": 0.5257575757575759,\n        \"temoin:100\": 1.0\n    },\n    \"num_ground_truth\": 141,\n    \"num_predictions\": 275,\n    \"class_names\": [\n        \"temoin:0\",\n        \"temoin:25\",\n        \"temoin:50\",\n        \"temoin:75\",\n        \"temoin:80\",\n        \"temoin:90\",\n        \"temoin:95\",\n        \"temoin:100\"\n    ]\n}\n","output_type":"stream"}],"execution_count":5}]}