{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13697018,"sourceType":"datasetVersion","datasetId":8712503},{"sourceId":13698332,"sourceType":"datasetVersion","datasetId":8713474},{"sourceId":13711372,"sourceType":"datasetVersion","datasetId":8722728}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q --no-cache-dir \\\n  numpy==1.26.4 scipy==1.11.4 \\\n  albumentations==1.4.8 albucore==0.0.12 \\\n  opencv-python-headless==4.10.0.84 pycocotools==2.0.10 \\\n  matplotlib==3.8.4 scikit-learn==1.3.2 \\\n  tqdm==4.67.1 bs4==0.0.2 lxml==5.2.2 isort==5.12.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T21:39:03.512440Z","iopub.execute_input":"2025-11-14T21:39:03.512646Z","iopub.status.idle":"2025-11-14T21:40:01.225216Z","shell.execute_reply.started":"2025-11-14T21:39:03.512625Z","shell.execute_reply":"2025-11-14T21:40:01.224262Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m206.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m305.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m146.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m130.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m200.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m189.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m188.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.3.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip uninstall -y albumentations albucore\n!pip install -q --no-cache-dir albumentations==1.4.8 albucore==0.0.12","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T21:40:01.226978Z","iopub.execute_input":"2025-11-14T21:40:01.227210Z","iopub.status.idle":"2025-11-14T21:40:06.054101Z","shell.execute_reply.started":"2025-11-14T21:40:01.227186Z","shell.execute_reply":"2025-11-14T21:40:06.053292Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: albumentations 1.4.8\nUninstalling albumentations-1.4.8:\n  Successfully uninstalled albumentations-1.4.8\nFound existing installation: albucore 0.0.12\nUninstalling albucore-0.0.12:\n  Successfully uninstalled albucore-0.0.12\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from pathlib import Path\nimport os\n\ndef resolve_path(local: str, kaggle_dataset: str, *extra: str) -> str:\n    local_path = Path(local, *extra)\n    if local_path.exists():\n        return str(local_path)\n\n    kaggle_base = Path('/kaggle/input') / kaggle_dataset\n    kaggle_path = kaggle_base.joinpath(*extra)\n    if kaggle_path.exists():\n        return str(kaggle_path)\n\n    raise FileNotFoundError(f\"Impossible de trouver {local_path} ni {kaggle_path}\")\n\nis_kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE') is not None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T21:40:06.055176Z","iopub.execute_input":"2025-11-14T21:40:06.055515Z","iopub.status.idle":"2025-11-14T21:40:06.061280Z","shell.execute_reply.started":"2025-11-14T21:40:06.055485Z","shell.execute_reply":"2025-11-14T21:40:06.060725Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n\nimport json\nimport xml.etree.ElementTree as ET\nfrom collections import defaultdict\nfrom typing import Dict, List, Tuple\n\nimport albumentations as A\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom albumentations.pytorch import ToTensorV2\nfrom PIL import Image, ImageDraw, ImageFont\nfrom pycocotools.coco import COCO\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.models.detection import _utils as model_utils\nfrom torchvision.models.detection import ssd300_vgg16\nfrom torchvision.models.detection.ssd import SSD300_VGG16_Weights, SSDClassificationHead\nfrom torchvision.models.vgg import VGG16_Weights\nfrom tqdm import tqdm\n\n\n# -----------------------------\n# Datasets\n# -----------------------------\n\nclass COCODataset(Dataset):\n    \"\"\"COCO format dataset returning (image_tensor, target_dict).\"\"\"\n    def __init__(self, root_dir: str, annotation_file: str, transforms=None, min_area: float = 0.0):\n        self.root_dir = root_dir\n        self.coco = COCO(annotation_file)\n        self.image_ids = [img_id for img_id in self.coco.imgs.keys() if len(self.coco.getAnnIds(imgIds=img_id)) > 0]\n        self.cat_ids = self.coco.getCatIds()\n        self.cat_id_to_idx = {cat_id: i + 1 for i, cat_id in enumerate(self.cat_ids)}  # 0 is background\n        self.class_names = [c['name'] for c in self.coco.loadCats(self.cat_ids)]\n        self.transforms = transforms\n        self.min_area = min_area\n        print(f\"COCO dataset loaded: {len(self.image_ids)} images, {len(self.class_names)} classes\")\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx: int):\n        image_id = self.image_ids[idx]\n        img_info = self.coco.loadImgs(image_id)[0]\n        img_path = os.path.join(self.root_dir, img_info['file_name'])\n        image = Image.open(img_path).convert('RGB')\n        w, h = image.size\n\n        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n        anns = self.coco.loadAnns(ann_ids)\n\n        boxes = []\n        labels = []\n        areas = []\n        iscrowd = []\n        for ann in anns:\n            if ann['area'] < self.min_area:\n                continue\n            x, y, bw, bh = ann['bbox']\n            x1, y1, x2, y2 = x, y, x + bw, y + bh\n            # clamp\n            x1 = max(0, min(x1, w - 1))\n            y1 = max(0, min(y1, h - 1))\n            x2 = max(x1 + 1, min(x2, w))\n            y2 = max(y1 + 1, min(y2, h))\n            bw2 = x2 - x1\n            bh2 = y2 - y1\n            area = bw2 * bh2\n            if area < self.min_area or bw2 < 2 or bh2 < 2:\n                continue\n            boxes.append([x1, y1, x2, y2])\n            labels.append(self.cat_id_to_idx[ann['category_id']])\n            areas.append(area)\n            iscrowd.append(ann.get('iscrowd', 0))\n\n        if len(boxes) == 0:\n            # Skip images without usable boxes by picking another index (rare in filtered list)\n            return self.__getitem__((idx + 1) % len(self))\n\n        # Albumentations\n        if self.transforms:\n            transformed = self.transforms(image=np.array(image), bboxes=boxes, labels=labels)\n            img_tensor = transformed['image']\n            boxes = transformed['bboxes']\n            labels = transformed['labels']\n        else:\n            # Basic conversion\n            from torchvision import transforms as T\n            img_tensor = T.ToTensor()(image)\n\n        boxes_tensor = torch.tensor(boxes, dtype=torch.float32)\n        labels_tensor = torch.tensor(labels, dtype=torch.int64)\n        areas_tensor = torch.tensor(areas, dtype=torch.float32)\n        iscrowd_tensor = torch.tensor(iscrowd, dtype=torch.uint8)\n\n        target = {\n            'boxes': boxes_tensor,\n            'labels': labels_tensor,\n            'image_id': torch.tensor([image_id]),\n            'area': areas_tensor,\n            'iscrowd': iscrowd_tensor\n        }\n        return img_tensor, target\n\n\nclass UsureDataset(Dataset):\n    \"\"\"VOC-style XML dataset used in original script.\"\"\"\n    def __init__(self, list_file: str, images_dir: str, transforms=None, class_names: List[str] | None = None, min_area: float = 0.0):\n        if not os.path.isfile(list_file):\n            raise FileNotFoundError(list_file)\n        if not os.path.isdir(images_dir):\n            raise NotADirectoryError(images_dir)\n\n        self.images_dir = images_dir\n        with open(list_file, 'r', encoding='utf-8') as f:\n            self.image_names = [l.strip() for l in f if l.strip()]\n        self.transforms = transforms\n        self.min_area = min_area\n\n        if class_names is None:\n            labels = set()\n            for name in self.image_names:\n                xml_path = os.path.join(images_dir, f\"{name}.xml\")\n                if not os.path.isfile(xml_path):\n                    continue\n                try:\n                    root = ET.parse(xml_path).getroot()\n                    for obj in root.findall('object'):\n                        n = obj.findtext('name')\n                        if n:\n                            labels.add(n.strip())\n                except Exception:\n                    continue\n            self.class_names = sorted(labels)\n        else:\n            self.class_names = class_names\n        self.cls_to_idx = {c: i + 1 for i, c in enumerate(self.class_names)}\n        print(f\"Usure dataset loaded: {len(self.image_names)} images, {len(self.class_names)} classes\")\n\n    def __len__(self):\n        return len(self.image_names)\n\n    def _parse_xml(self, xml_path: str, w: int, h: int):\n        boxes, labels, areas, iscrowd = [], [], [], []\n\n        try:\n            root = ET.parse(xml_path).getroot()\n        except Exception:\n            return boxes, labels, areas, iscrowd\n\n        for obj in root.findall('object'):\n            name = obj.findtext('name')\n            if not name or name.strip() not in self.cls_to_idx:\n                continue\n            bnd = obj.find('bndbox')\n            if bnd is None:\n                continue\n\n            try:\n                xmin = float(bnd.findtext('xmin', '0'))\n                ymin = float(bnd.findtext('ymin', '0'))\n                xmax = float(bnd.findtext('xmax', '0'))\n                ymax = float(bnd.findtext('ymax', '0'))\n            except Exception:\n                continue\n            xmin = max(0, min(xmin, w - 1))\n            ymin = max(0, min(ymin, h - 1))\n            xmax = max(xmin + 1, min(xmax, w))\n            ymax = max(ymin + 1, min(ymax, h))\n            bw = xmax - xmin\n            bh = ymax - ymin\n            area = bw * bh\n\n            if area < self.min_area or bw < 2 or bh < 2:\n                continue\n            boxes.append([xmin, ymin, xmax, ymax])\n            labels.append(self.cls_to_idx[name.strip()])\n            areas.append(area)\n            iscrowd.append(0)\n\n        return boxes, labels, areas, iscrowd\n\n    def __getitem__(self, idx: int):\n        name = self.image_names[idx]\n        img_path = os.path.join(self.images_dir, f\"{name}.jpg\")\n        xml_path = os.path.join(self.images_dir, f\"{name}.xml\")\n        image = Image.open(img_path).convert('RGB')\n        w, h = image.size\n\n        if os.path.isfile(xml_path):\n            boxes, labels, areas, iscrowd = self._parse_xml(xml_path, w, h)\n        else:\n            boxes, labels, areas, iscrowd = [], [], [], []\n\n        if len(boxes) == 0:\n            # skip empty annotation samples to keep training stable\n            return self.__getitem__((idx + 1) % len(self))\n\n        if self.transforms:\n            transformed = self.transforms(image=np.array(image), bboxes=boxes, labels=labels)\n            img_tensor = transformed['image']\n            boxes = transformed['bboxes']\n            labels = transformed['labels']\n        else:\n            from torchvision import transforms as T\n            img_tensor = T.ToTensor()(image)\n\n        target = {\n            'boxes': torch.tensor(boxes, dtype=torch.float32),\n            'labels': torch.tensor(labels, dtype=torch.int64),\n            'image_id': torch.tensor([idx]),\n            'area': torch.tensor(areas, dtype=torch.float32),\n            'iscrowd': torch.tensor(iscrowd, dtype=torch.uint8)\n        }\n\n        return img_tensor, target\n\n\n# -----------------------------\n# Model utilities\n# -----------------------------\n\ndef build_ssd(num_classes: int, freeze_backbone: bool = False, image_size: int = 300) -> torch.nn.Module:\n    # Download model when calling for the first time\n    model = ssd300_vgg16(weights=SSD300_VGG16_Weights.DEFAULT, weights_backbone=VGG16_Weights.DEFAULT)\n    in_channels = model_utils.retrieve_out_channels(model.backbone, (image_size, image_size))\n    num_anchors = model.anchor_generator.num_anchors_per_location()\n    model.head.classification_head = SSDClassificationHead(\n        in_channels=in_channels,\n        num_anchors=num_anchors,\n        num_classes=num_classes,\n    )\n\n    model.transform.min_size = (image_size,)\n    model.transform.max_size = image_size\n\n    if freeze_backbone:\n        for p in model.backbone.parameters():\n            p.requires_grad = False\n\n    return model\n\n# -----------------------------\n# Training / Validation loops\n# -----------------------------\n\ndef detection_collate(batch: List[Tuple[torch.Tensor, Dict]]):\n    images, targets = list(zip(*batch))\n    # Images already resized to same size, but detection model expects list[Tensor]\n    return list(images), list(targets)\n\ndef train_one_epoch(model, dataloader, optimizer, device, epoch: int):\n    model.train()\n    total, count = 0.0, 0\n\n    pbar = tqdm(dataloader, desc=f\"Train {epoch}\", unit='batch')\n    for images, targets in pbar:\n        images = [img.to(device) for img in images]\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        optimizer.zero_grad()\n\n        try:\n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n            if torch.isfinite(losses):\n                losses.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n                total += losses.item()\n                count += 1\n                pbar.set_postfix(loss=f\"{losses.item():.4f}\", avg=f\"{(total/max(count,1)):.4f}\")\n        except Exception as e:\n            print(f\"Batch error: {e}\")\n            continue\n\n    return total / max(count, 1)\n\n\n# ===== Metrics Helper Functions (dataset agnostic) =====\n\ndef _iou_matrix_np(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    if a.size == 0 or b.size == 0:\n        return np.zeros((a.shape[0], b.shape[0]), dtype=np.float32)\n\n    ax1, ay1, ax2, ay2 = a[:,0], a[:,1], a[:,2], a[:,3]\n    bx1, by1, bx2, by2 = b[:,0], b[:,1], b[:,2], b[:,3]\n\n    inter_x1 = np.maximum(ax1[:,None], bx1[None,:])\n    inter_y1 = np.maximum(ay1[:,None], by1[None,:])\n    inter_x2 = np.minimum(ax2[:,None], bx2[None,:])\n    inter_y2 = np.minimum(ay2[:,None], by2[None,:])\n\n    iw = np.clip(inter_x2 - inter_x1, 0, None)\n    ih = np.clip(inter_y2 - inter_y1, 0, None)\n\n    inter = iw * ih\n    area_a = (ax2 - ax1) * (ay2 - ay1)\n    area_b = (bx2 - bx1) * (by2 - by1)\n    union = area_a[:,None] + area_b[None,:] - inter\n\n    return inter / np.clip(union, 1e-8, None)\n\ndef _integrate_pr(recalls: np.ndarray, precisions: np.ndarray) -> float:\n    if recalls.size == 0:\n        return 0.0\n\n    mrec = np.concatenate(([0.0], recalls, [1.0]))\n    mpre = np.concatenate(([0.0], precisions, [0.0]))\n\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i-1] = max(mpre[i-1], mpre[i])\n    idx = np.where(mrec[1:] != mrec[:-1])[0]\n\n    return float(np.sum((mrec[idx+1] - mrec[idx]) * mpre[idx+1]))\n\ndef _compute_coco_ap(gt_boxes, preds_per_class):\n    \"\"\"\n    gt_boxes: dict[(img_id, cls)] -> list[np.array(4)]\n    preds_per_class: dict[cls] -> list{score, img_id, box}\n    Returns: overall_map, map_per_iou(dict), ap50, ap75\n    \"\"\"\n    iou_thresholds = [round(x/100, 2) for x in range(50, 100, 5)]\n    class_ids = sorted({cls for (_im, cls) in gt_boxes.keys()})\n    gt_count_per_class = {cls: 0 for cls in class_ids}\n    for (img_id, cls), lst in gt_boxes.items():\n        gt_count_per_class[cls] += len(lst)\n\n    ap_per_iou = {thr: [] for thr in iou_thresholds}\n\n    for thr in iou_thresholds:\n        for cls in class_ids:\n            n_gt = gt_count_per_class[cls]\n            if n_gt == 0:\n                continue\n\n            preds = preds_per_class.get(cls, [])\n            preds_sorted = sorted(preds, key=lambda d: d[\"score\"], reverse=True)\n\n            # fresh matched flags per (img, cls)\n            matched_flags = {}\n            for (img_id, c), lst in gt_boxes.items():\n                if c == cls:\n                    matched_flags[(img_id, c)] = [False]*len(lst)\n\n            tp, fp = [], []\n            for pred in preds_sorted:\n                img_id = pred[\"img_id\"]\n                key = (img_id, cls)\n                matched = False\n                if key in gt_boxes:\n                    g = np.vstack(gt_boxes[key])\n                    ious = _iou_matrix_np(pred[\"box\"][None,:], g)[0]\n                    best = np.argmax(ious) if ious.size else -1\n                    if best >= 0 and ious[best] >= thr and not matched_flags[key][best]:\n                        matched_flags[key][best] = True\n                        matched = True\n                tp.append(1 if matched else 0)\n                fp.append(0 if matched else 1)\n\n            if tp:\n                tp_cum = np.cumsum(tp)\n                fp_cum = np.cumsum(fp)\n                recalls = tp_cum / max(1, n_gt)\n                precisions = tp_cum / np.maximum(1, tp_cum + fp_cum)\n                ap = _integrate_pr(recalls, precisions)\n            else:\n                ap = 0.0\n            ap_per_iou[thr].append(ap)\n\n    map_per_iou = {thr: (float(np.mean(v)) if v else 0.0) for thr, v in ap_per_iou.items()}\n    overall_map = float(np.mean(list(map_per_iou.values()))) if map_per_iou else 0.0\n    ap50 = map_per_iou.get(0.5, 0.0)\n    ap75 = map_per_iou.get(0.75, 0.0)\n\n    return overall_map, map_per_iou, ap50, ap75, gt_count_per_class, class_ids\n\ndef _compute_micro_auprc(gt_boxes, preds_per_class, gt_count_per_class, iou_thr=0.5):\n    \"\"\"\n    Micro-averaged AUPRC (area under precision-recall curve) across all classes at a single IoU\n    threshold.\n    \"\"\"\n    total_gt = sum(gt_count_per_class.values())\n    if total_gt == 0:\n        return 0.0\n\n    # Flatten predictions\n    flat = []\n    for cls, plist in preds_per_class.items():\n        for p in plist:\n            flat.append((p[\"score\"], cls, p[\"img_id\"], p[\"box\"]))\n    flat.sort(key=lambda x: x[0], reverse=True)\n\n    matched_flags = {}\n    for k, lst in gt_boxes.items():\n        matched_flags[k] = [False]*len(lst)\n\n    tp_run = 0\n    fp_run = 0\n    precisions = []\n    recalls = []\n    for sc, cls, img_id, box in flat:\n        key = (img_id, cls)\n        matched = False\n        if key in gt_boxes:\n            g = np.vstack(gt_boxes[key])\n            ious = _iou_matrix_np(box[None,:], g)[0]\n            best = np.argmax(ious) if ious.size else -1\n            if best >= 0 and ious[best] >= iou_thr and not matched_flags[key][best]:\n                matched_flags[key][best] = True\n                matched = True\n        if matched:\n            tp_run += 1\n        else:\n            fp_run += 1\n        precisions.append(tp_run / max(1, tp_run + fp_run))\n        recalls.append(tp_run / total_gt)\n\n    if not recalls:\n        return 0.0\n    r = np.array(recalls); p = np.array(precisions)\n    order = np.argsort(r)\n    r = r[order]; p = p[order]\n\n    for i in range(p.size - 1, 0, -1):\n        p[i-1] = max(p[i-1], p[i])\n\n    return float(np.trapz(p, r))\n\ndef _forward_and_collect(model, images, targets, device, gt_boxes, preds_per_class):\n    images = [img.to(device) for img in images]\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n    # loss (train mode)\n    loss_dict = model(images, targets)\n    losses = sum(v for v in loss_dict.values())\n\n    # predictions (eval mode)\n    model.eval()\n    det_outs = model(images)\n    model.train()\n\n    for det, tgt in zip(det_outs, targets):\n        img_id = int(tgt['image_id'].item()) if tgt['image_id'].numel() == 1 else int(tgt['image_id'][0].item())\n\n        # GT\n        gboxes = tgt['boxes'].detach().cpu().numpy()\n        glabels = tgt['labels'].detach().cpu().numpy()\n        for gbox, glab in zip(gboxes, glabels):\n            if glab == 0:\n                continue\n            gt_boxes[(img_id, int(glab))].append(gbox.astype(np.float32))\n\n        # Preds\n        pboxes = det['boxes'].detach().cpu().numpy()\n        pscores = det['scores'].detach().cpu().numpy()\n        plabels = det['labels'].detach().cpu().numpy()\n        for box, sc, lab in zip(pboxes, pscores, plabels):\n            if lab == 0:\n                continue\n            preds_per_class[int(lab)].append({\n                \"score\": float(sc),\n                \"img_id\": img_id,\n                \"box\": box.astype(np.float32)\n            })\n\n    return losses\n\ndef validate(model, dataloader, device, epoch : int | None = None):\n    # Need model in training mode for it to return loss dict (torchvision detection API)\n    was_training = model.training\n    model.train()\n    total_loss = 0.0\n    count = 0\n\n    gt_boxes = defaultdict(list)       # (img_id, cls) -> list[box]\n    preds_per_class = defaultdict(list)\n    \n    if epoch is not None:\n        pbar = tqdm(dataloader, desc=f\"Val   {epoch}\", unit=\"batch\")\n    else:\n        pbar = tqdm(dataloader, desc=\"Val\", unit=\"batch\")\n\n    with torch.no_grad():\n        for images, targets in pbar:\n            if any(t['boxes'].numel() == 0 for t in targets):\n                continue\n            try:\n                losses = _forward_and_collect(model, images, targets, device, gt_boxes, preds_per_class)\n                if torch.isfinite(losses):\n                    total_loss += losses.item()\n                    count += 1\n                    pbar.set_postfix(val_loss=f\"{losses.item():.4f}\", avg=f\"{(total_loss/max(count,1)):.4f}\")\n            except Exception as e:\n                pbar.set_postfix(error=str(e))\n                continue\n\n    # Metrics\n    overall_map, map_per_iou, ap50, ap75, gt_count_per_class, class_ids = _compute_coco_ap(gt_boxes, preds_per_class)\n    auprc = _compute_micro_auprc(gt_boxes, preds_per_class, gt_count_per_class, iou_thr=0.5)\n\n    if epoch is not None:\n        print(f\"[Epoch {epoch}] COCO mAP(0.50:0.95) {overall_map:.4f} | AP50 {ap50:.4f} | AP75 {ap75:.4f} | Micro AUPRC@0.5 {auprc:.4f}\")\n    else:\n        print(f\"COCO mAP(0.50:0.95) {overall_map:.4f} | AP50 {ap50:.4f} | AP75 {ap75:.4f} | Micro AUPRC@0.5 {auprc:.4f}\")\n\n    if not was_training:\n        model.eval()\n\n    return total_loss / max(count, 1)\n\n\n# -----------------------------\n# Prediction & Visualization\n# -----------------------------\n\ndef plot_curves(train_losses: List[float], val_losses: List[float], out_path: str):\n    plt.figure(figsize=(12,5))\n    plt.subplot(1,2,1)\n    plt.plot(train_losses, label='Train')\n    plt.plot(val_losses, label='Val')\n    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss Curves'); plt.grid(True); plt.legend()\n    plt.subplot(1,2,2)\n    plt.plot(train_losses, label='Train')\n    plt.plot(val_losses, label='Val')\n    plt.yscale('log'); plt.xlabel('Epoch'); plt.ylabel('Loss (log)'); plt.title('Loss (Log)'); plt.grid(True); plt.legend()\n    plt.tight_layout()\n    plt.savefig(out_path, dpi=300)\n    plt.close()\n    print(f\"Saved curves to {out_path}\")\n\n# ----- Clean predict_image & visualize -----\ndef predict_image(model,\n                  image_path: str,\n                  device: str,\n                  class_names: List[str],\n                  transform=None,\n                  conf_thresh: float = 0.5):\n    model.eval()\n    pil_image = Image.open(image_path).convert('RGB')\n    orig_w, orig_h = pil_image.size\n\n    if transform is not None:\n        transformed = transform(image=np.array(pil_image), bboxes=[], labels=[])\n        img_tensor = transformed['image']\n    else:\n        basic = A.Compose([\n            A.Resize(300, 300),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            ToTensorV2()\n        ])\n        img_tensor = basic(image=np.array(pil_image))['image']\n\n    resized_h, resized_w = img_tensor.shape[1], img_tensor.shape[2]\n    with torch.no_grad():\n        out = model([img_tensor.to(device)])[0]\n\n    scores = out['scores'].cpu().numpy()\n    boxes = out['boxes'].cpu().numpy()\n    labels = out['labels'].cpu().numpy()\n    keep = scores >= conf_thresh\n    scores, boxes, labels = scores[keep], boxes[keep], labels[keep]\n\n    if (orig_w, orig_h) != (resized_w, resized_h):\n        sx = orig_w / resized_w\n        sy = orig_h / resized_h\n        boxes[:, [0, 2]] *= sx\n        boxes[:, [1, 3]] *= sy\n\n    return {\n        'scores': scores,\n        'boxes': boxes,\n        'labels': labels,\n        'orig_size': (orig_w, orig_h),\n        'proc_size': (resized_w, resized_h)\n    }\n\n\ndef visualize_predictions(image_path: str, predictions: Dict, class_names: List[str], save_path: str | None = None):\n    image = Image.open(image_path).convert('RGB')\n    draw = ImageDraw.Draw(image)\n\n    try:\n        font = ImageFont.truetype(\"arial.ttf\", 16)\n    except Exception:\n        font = ImageFont.load_default()\n    colors = ['red','blue','green','orange','purple','yellow','cyan','magenta','lime','pink']\n\n    for box, label, score in zip(predictions['boxes'], predictions['labels'], predictions['scores']):\n        x1, y1, x2, y2 = box\n        cname = class_names[label - 1] if 0 < label <= len(class_names) else f\"cls_{label}\"\n        color = colors[label % len(colors)]\n        # outline\n        draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n\n        text = f\"{cname}:{score:.2f}\"\n        # Proper bbox for text\n        tx1, ty1, tx2, ty2 = draw.textbbox((x1, y1), text, font=font)\n        # Shift label above box if space, else draw inside\n        label_bottom = y1\n        label_top = label_bottom - (ty2 - ty1)\n        if label_top < 0:\n            label_top = y1\n            label_bottom = y1 + (ty2 - ty1)\n        draw.rectangle([tx1, label_top, tx2, label_bottom], fill=color)\n        draw.text((tx1, label_top), text, fill='white', font=font)\n\n    if save_path:\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        image.save(save_path)\n        print(f\"Saved prediction image -> {save_path}\")\n    return image\n\n\n# -----------------------------\n# Wrapper combining training & validation with curve saving\n# -----------------------------\n\ndef train_and_evaluate(model: torch.nn.Module,\n                       train_loader: DataLoader,\n                       val_loader: DataLoader,\n                       optimizer: torch.optim.Optimizer,\n                       scheduler: torch.optim.lr_scheduler._LRScheduler | None,\n                       device: str,\n                       epochs: int,\n                       output_dir: str,\n                       prefix: str):\n    os.makedirs(output_dir, exist_ok=True)\n    train_losses: List[float] = []\n    val_losses: List[float] = []\n\n    for epoch in range(epochs):\n        tr = train_one_epoch(model, train_loader, optimizer, device, epoch + 1)\n        va = validate(model, val_loader, device, epoch + 1)\n        if scheduler:\n            scheduler.step()\n        train_losses.append(tr)\n        val_losses.append(va)\n        # intermediate checkpoint every 5 epochs\n        if (epoch + 1) % 5 == 0 or (epoch + 1) == epochs:\n            torch.save({'model_state_dict': model.state_dict()}, os.path.join(output_dir, f'{prefix}_epoch_{epoch+1}.pth'))\n\n    if epochs > 0:\n        plot_curves(train_losses, val_losses, os.path.join(output_dir, f'{prefix}_curves.png'))\n        torch.save({'model_state_dict': model.state_dict()}, os.path.join(output_dir, f'{prefix}_final.pth'))\n\n    return train_losses, val_losses\n\n\n# -----------------------------\n# Main pipeline\n# -----------------------------\n\ndef main():\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print(f\"Using device: {device}\")\n\n    # ======================================================\n    # COCO DATASET\n    # ======================================================\n    print(\"\\n=== Loading COCO subset ===\")\n\n    from pathlib import Path\n\n    kaggle_root = Path('/kaggle/input/coco2014/subset_coco')\n    local_root = Path('subset_coco')\n    coco_root = kaggle_root if kaggle_root.exists() else local_root\n    \n    train_images_dir = str(coco_root / 'subset_train_images_2014')\n    val_images_dir   = str(coco_root / 'subset_valid_images_2014')\n    test_images_dir  = str(coco_root / 'test_image_info_2014')\n    train_ann        = str(coco_root / 'train_val_annotations' / 'subset_instances_train2014.json')\n    val_ann          = str(coco_root / 'train_val_annotations' / 'subset_instances_val2014.json')\n    test_image_name  = 'COCO_test2014_000000000001.jpg'\n    saved_model_path = '/kaggle/working/outputs/ssd300_coco_final.pth'\n\n    # Paths (subset versions for faster experimentation)\n    # train_images_dir = 'subset_coco/subset_train_images_2014'\n    # val_images_dir   = 'subset_coco/subset_val_images_2014'\n    # test_images_dir  = 'subset_coco/test_images_2014'\n    # train_ann        = 'subset_coco/train_val_annotations/subset_instances_train2014.json'\n    # val_ann          = 'subset_coco/train_val_annotations/subset_instances_val2014.json'\n    # saved_model_path = 'outputs/ssd300_coco_final.pth'\n    # test_image_name  = 'COCO_test2014_000000000001.jpg'\n\n    # Inline transforms (train / val)\n    image_size = 300\n\n    train_transform = A.Compose([\n        A.Resize(image_size, image_size),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2()\n    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], min_area=1, min_visibility=0.0))\n\n    val_transform = A.Compose([\n        A.Resize(image_size, image_size),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2()\n    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], min_area=1, min_visibility=0.0))\n\n    # Datasets / loaders\n    coco_train = COCODataset(train_images_dir, train_ann, transforms=train_transform, min_area=32*32)\n    coco_val   = COCODataset(val_images_dir,   val_ann,   transforms=val_transform,   min_area=32*32)\n    num_classes_coco = len(coco_train.class_names) + 1  # + background\n    print(f\"COCO classes ({num_classes_coco-1}): {coco_train.class_names}\")\n\n    train_loader = DataLoader(coco_train, batch_size=8, shuffle=True,  collate_fn=detection_collate, num_workers=0)\n    val_loader   = DataLoader(coco_val,   batch_size=8, shuffle=False, collate_fn=detection_collate, num_workers=0)\n\n    # Model + optim\n    model = build_ssd(num_classes_coco, freeze_backbone=True).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n\n    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 15], gamma=0.1)\n\n    # Training epochs\n    epochs_coco = 1\n\n    if epochs_coco > 0:\n        train_and_evaluate(model, train_loader, val_loader, optimizer, scheduler, device, epochs_coco, 'outputs', 'ssd300_coco')\n\n    # prediction on a test image\n    test_image_path = test_images_dir + test_image_name\n    if os.path.exists(test_image_path):\n        print(\"COCO prediction test...\")\n        preds = predict_image(model, test_image_path, device, coco_train.class_names,\n                              transform=val_transform, conf_thresh=0.7)\n        visualize_predictions(test_image_path, preds, coco_train.class_names, save_path='outputs/prediction_coco.jpg')\n        print(f\"Detections: {len(preds['labels'])}\")\n        for l, s in zip(preds['labels'], preds['scores']):\n            cname = coco_train.class_names[l - 1] if l > 0 else 'bg'\n            print(f\"  {cname}: {s:.3f}\")\n\n    # inference using a checkpoint\n    model_checkpoint_path = saved_model_path\n    model = build_ssd(num_classes_coco, freeze_backbone=True).to(device)\n    if os.path.isfile(model_checkpoint_path):\n        model.load_state_dict(torch.load(model_checkpoint_path, map_location=device)['model_state_dict'])\n        print(f\"Loaded checkpoint from {model_checkpoint_path}\")\n    \n    val_loss = validate(model, val_loader, device)\n    \n\n    # ======================================================\n    # USURE DATASET\n    # ======================================================\n\n    from pathlib import Path\n    \n    print(\"\\n=== Loading Usure dataset ===\")\n\n    from pathlib import Path\n    \n    usure_slug = 'data-tu/data_tu'\n    local_usure = Path('data_tu')\n    kaggle_usure = Path('/kaggle/input') / usure_slug\n    usure_root = kaggle_usure if kaggle_usure.exists() else local_usure\n    \n    usure_train_list = str(usure_root / 'image_names_train.txt')\n    usure_val_list   = str(usure_root / 'image_names_val.txt')\n    usure_images_dir = str(usure_root / 'image_base')\n\n\n    # usure_train_list  = 'data_tu/image_names_train.txt'\n    # usure_val_list    = 'data_tu/image_names_val.txt'\n    # usure_images_dir  = 'data_tu/image_base'\n\n    from pathlib import Path\n    import json\n    \n    param_slug = 'utilities'  # nom du dataset Kaggle\n    local_params = Path('utilities/parameters.json')\n    kaggle_params = Path('/kaggle/input') / param_slug / 'parameters.json'  # ✅ corrigé ici\n    param_path = kaggle_params if kaggle_params.exists() else local_params\n    \n    print(\"[Info] Param path:\", param_path)\n    \n    with open(param_path, 'r', encoding='utf-8') as f:\n        params = json.load(f)\n    \n    class_filter = params.get('class_filter')\n\n\n    # # Load class filter (optional) from parameters\n    # with open('utilities/parameters.json', 'r', encoding='utf-8') as f:\n    #     params = json.load(f)\n    # class_filter = params.get('class_filter', None)\n\n    # Datasets / loaders\n    usure_train = UsureDataset(usure_train_list, usure_images_dir, transforms=train_transform, class_names=class_filter, min_area=32*32)\n    usure_val   = UsureDataset(usure_val_list,   usure_images_dir, transforms=val_transform,   class_names=class_filter, min_area=32*32)\n    num_usure_classes = len(usure_train.class_names) + 1\n    print(f\"Usure classes ({num_usure_classes-1}): {usure_train.class_names}\")\n\n    usure_train_loader = DataLoader(usure_train, batch_size=8, shuffle=True,  collate_fn=detection_collate, num_workers=0)\n    usure_val_loader   = DataLoader(usure_val,   batch_size=8, shuffle=False, collate_fn=detection_collate, num_workers=0)\n\n    # Build a fresh model for Usure\n    model_usure = build_ssd(num_usure_classes, freeze_backbone=False).to(device)\n\n    optimizer_u = torch.optim.AdamW(model_usure.parameters(), lr=1e-4, weight_decay=1e-4)\n    scheduler_u = torch.optim.lr_scheduler.MultiStepLR(optimizer_u, milestones=[10, 15], gamma=0.1)\n\n    # Short demo run; extend epochs for real training\n    epochs_usure = 1\n    train_and_evaluate(model_usure, usure_train_loader, usure_val_loader, optimizer_u, scheduler_u, device, epochs_usure, 'outputs', 'ssd300_usure')\n\n    # Single prediction on first validation sample\n    with open(usure_val_list, 'r', encoding='utf-8') as f:\n        first_name = f.readline().strip()\n    usure_test_image = os.path.join(usure_images_dir, f\"{first_name}.jpg\")\n    if os.path.exists(usure_test_image):\n        print(\"Usure prediction test...\")\n        preds_u = predict_image(model_usure, usure_test_image, device, usure_train.class_names,\n                                transform=val_transform, conf_thresh=0.2)\n        visualize_predictions(usure_test_image, preds_u, usure_train.class_names, save_path='outputs/prediction_usure.jpg')\n        print(f\"Usure detections: {len(preds_u['labels'])}\")\n        for l, s in zip(preds_u['labels'], preds_u['scores']):\n            cname = usure_train.class_names[l - 1] if l > 0 else 'bg'\n            print(f\"  {cname}: {s:.3f}\")\n\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:04:30.056416Z","iopub.execute_input":"2025-11-14T22:04:30.056782Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\n=== Loading COCO subset ===\nloading annotations into memory...\nDone (t=2.13s)\ncreating index...\nindex created!\nCOCO dataset loaded: 8208 images, 80 classes\nloading annotations into memory...\nDone (t=0.55s)\ncreating index...\nindex created!\nCOCO dataset loaded: 4013 images, 80 classes\nCOCO classes (80): ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n","output_type":"stream"},{"name":"stderr","text":"Train 1: 100%|██████████| 1026/1026 [04:07<00:00,  4.15batch/s, avg=7.9150, loss=5.2813] \nVal   1: 100%|██████████| 502/502 [04:10<00:00,  2.00batch/s, avg=4.8043, val_loss=3.8894]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] COCO mAP(0.50:0.95) 0.1663 | AP50 0.3875 | AP75 0.1160 | Micro AUPRC@0.5 0.4863\nSaved curves to outputs/ssd300_coco_curves.png\nLoaded checkpoint from /kaggle/working/outputs/ssd300_coco_final.pth\n","output_type":"stream"},{"name":"stderr","text":"Val: 100%|██████████| 502/502 [04:04<00:00,  2.06batch/s, avg=4.8043, val_loss=3.8894]\n","output_type":"stream"},{"name":"stdout","text":"COCO mAP(0.50:0.95) 0.1663 | AP50 0.3875 | AP75 0.1160 | Micro AUPRC@0.5 0.4863\n\n=== Loading Usure dataset ===\n[Info] Param path: /kaggle/input/utilities/parameters.json\nUsure dataset loaded: 1151 images, 8 classes\nUsure dataset loaded: 288 images, 8 classes\nUsure classes (8): ['temoin:0', 'temoin:25', 'temoin:50', 'temoin:75', 'temoin:80', 'temoin:90', 'temoin:95', 'temoin:100']\n","output_type":"stream"},{"name":"stderr","text":"Train 1:  87%|████████▋ | 125/144 [00:56<00:08,  2.16batch/s, avg=7.2361, loss=4.3231] ","output_type":"stream"}],"execution_count":null}]}